---
title: "Untitled"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, read_books_and_tidy}
# Read in goodreads data gather from their data export page
# https://www.goodreads.com/review/import
get_tidy_books <- function(file){
  # load required packages
  library(readr)
  library(dplyr)
  library(snakecase)
  library(lubridate)
  library(stringr)
  # read in raw data file: csv when you import your books. May change using goodreads API
  books <- read_csv(file)
  
  # change column names to an easier to work with format
  colnames(books) <- to_any_case(colnames(books), case = "snake") 
  
  #Cols to keep
  keeper_cols <- c("book_id", "title", "author", "my_rating", "number_of_pages", 
                   "original_publication_year", "date_read", "date_added", "bookshelves", "exclusive_shelf")
  
  # only looking at books users already read, not books on the to-read or books you want to read shelves.
  read_books <- books %>%
    filter(exclusive_shelf == "read")%>%
    select(one_of(keeper_cols))%>%
    # create some useful date columns and make sure date_read is in the format I want
    mutate(date_read = ymd(date_read))%>%
    mutate(year_read = year(date_read))%>%
    arrange(desc(date_read))
  
  # tidy book names so they don't include the stuff in paraentheses --it messes with the classify pages
  read_books %>%
    # removes book info inside paraentheses and trim any whitespace left over
    mutate(title = str_trim(str_remove(title, " \\([^()]+\\)")))
  
}

books <- get_tidy_books("Data/goodreads_library_export.csv")
```



```{r, encoded_urls}
encode_all_urls <- function(data){
  # This function encodes urls for all the books included in the goodreads data export data frame (generated by get_tidy_books)
  library(dplyr)
  library(purrr)
  base <- "http://classify.oclc.org/classify2/Classify?"
  title_and_author <- data%>%
    select(title, author)%>%
    mutate(title_encoded = map_chr(title, URLencode),
           author_encoded = map_chr(author, URLencode))%>%
    mutate(urls = paste0(base, "author=", author_encoded, "&title=", title_encoded))
  title_and_author
}

urls <- encode_urls(books)
```



Now I have to start docker (I launch it with the Docker quickstart terminal--no idea if this is the best way). After docker is up and running I run the following command in a terminal. Since I'm on windows, I don't 'think' I can run this from a bash code chunk.

```{bash, eval = FALSE}
docker run -d -p 4445:4444 selenium/standalone-chrome
```

With that run, now I can use RSelenium and my standard R packages to scrape some information.


```{r}
library(httr)
library(xml2)
library(rvest)
library(RSelenium)
library(purrr)
library(dplyr)

test_copy <- urls
test_urls <- test_copy%>%
  slice(c(1, 2, 6, 47, 71,  82, 155))


#First I need to get the wi numbers for each author/title. I can do this with basic tidyverse scraping functionality
grab_wi_number <- function(author = NULL, title = NULL, .verbose = TRUE){
  # wi #'s are found in two different places: 
  #         - Most often in the 'work' element under the 'wi' attribute 
  #         - Occasionally in the xml text of the 'work' element (generally in non-fiction books it seems like)
  
  # Generate encoded url
  if(is.null(author) | is.null(title)){
    if(.verbose) message("Author or title not supplied")
    return(NA)
  }
  base_url <- "http://classify.oclc.org/classify2/Classify?"
  
  encoded_url <- paste0(base_url, "author=", URLencode(author), "&title=", URLencode(title))
  
  # Be nice the the classify website and don't bombard the api
  Sys.sleep(sample(seq(0, 1, .25), 1))
  
  # wrap the GET httr function with purrr::safely which gives it exception handling code
  sGET <- purrr::safely(GET)
  res <- sGET(encoded_url)
  
  # If the classify api doesn't find a result for the author+title, it returns an xml page with response code 102, otherwise it will be 2 or 4 I believe.
  # I want to grab this response code to use for checking if I have found the book I searched for
  response_code <- read_html(res$result)%>%
    html_nodes("response")%>%
    map(html_attrs)%>%
    simplify()%>%
    first()
  
  # checks 1) if the httr call returned NULL
  #        2) if the httr call came back OK (wasn't a 404 error code or something like that)
  #        3) if the xml has a 102 response code(meaning it didn't find the book and is basically empty xml)
  if(is.null(res$result) | (status_code(res$result) !=200) | response_code == 102){
    # if these conditions return TRUE, warn that somethings wrong with the url and return an NA value
    if(.verbose) message("URL invalid or results returned NULL")
    return(NA)
  } else{
    
    # read the valid xml page
    requested_url <- content(res$result, as = 'text')
    raw_html <- read_html(requested_url)
    
    
    # This handles scraping wi # from the two locations it can be found
    
    # Get a list of the work attributes found in the html
    work_attributes <- html_nodes(raw_html, "work")%>%
      map(html_attrs)
    
    # if wi is an attribute, I get the wi # from there and return it as a character vector
    if("wi" %in% attributes(work_attributes[[1]])$names){
      # scrape the WI number from the title, author input. 
      works_tdf <- work_attributes%>%
        # create a tibble out of the list you get
        map_df(~as.list(.))%>%
        # only need the WI column
        select(wi)%>%
        pull(wi)
      # return a single wi number as a character
      works_tdf[1]
      
    } else {
      # if wi is not an attribute for the xml file, then wi is listed as text in the work element
      html_nodes(raw_html, "work")%>%
        map(xml_text)%>%
        # extract the character vector containing the wi number
        simplify()%>%
        first()
    }
  }
  
}


# with the wi numbers, I can scrap the VIAF numbers I'll use to tag gender
get_viaf_number <- function(author = NULL, title = NULL, .verbose = TRUE){
  # Make sure an author and title are given
  if(is.null(author) | is.null(title)){
    if(.verbose) message("Author or title not supplied")
    return(NA)
  }
  
  # Need to check if wi # is NA. If so, return NA value
  wi_number <- grab_wi_number(author, title)
  if(is.na(wi_number)){
    if(.verbose) message("NA WI number. Book title+author not found")
    return(NA)
  }
  
  # Also need to make sure I'm getting the right author--translators and multiple authors can be picked by accident if I'm not careful.
  base_url <- "http://classify.oclc.org/classify2/ClassifyDemo?wi="
  
  # scrape the viaf table: May contain multiple authors/translators. Need to check and get the viaf # associated with the actual author
  viaf_table <- read_html(paste0(base_url, wi_number))%>%
    html_nodes("#display-V-tbl a")%>%
    #html_nodes("#display-V-tbl td+ td a")%>%=
    html_text()%>%
    matrix(., ncol = 2, 
                   byrow = TRUE,
                   dimnames = list(NULL, c("name", "viaf")))%>%
    as_tibble()%>%
    # Authors with one name (eg Voltaire) may or may not be extracted correctly. If they have a comma after the first name, this should work.
    # If no comma, then NA is returned
    tidyr::extract(name, "last", "([A-Za-z]+),")%>%
    filter(str_detect(author, last))%>%
    pull(viaf)
    
  
  # return the viaf number associated with the author argument
  
  viaf_table
}

  
library(stringr)
t_df <- as_tibble(matrix(t, ncol = 2, byrow = TRUE,
       dimnames = list(NULL, c("name", "viaf")))) %>%
  tidyr::extract(name, c("last"), "([A-Za-z]+),")


t2_df <- as_tibble(matrix(t2, ncol = 2, byrow = TRUE,
       dimnames = list(NULL, c("name", "viaf")))) %>%
  tidyr::extract(name, c("last"), "([A-Za-z]+),")
test_urls <- test_urls%>%
  mutate(is_author = str_detect(.$last))
fuzzyjoin


temp_small$viaf <- map_chr(temp_small$urls, get_viaf_number)

# Scraping Viaf website requires RSelenium, so 1) make sure docker is up and running
# 2) make sure R and the server are connected
# can't use remoteServerAddr = "localhost" for windows. Needs to be the ip of the VM that is running docker
remDr <- remoteDriver(
  remoteServerAddr = "192.168.99.100",
  port = 4445L,
  browserName = "chrome"
)
# don't think I want to instantiate the browser every time, so I do it once here
remDr$open()

get_gender <- function(viaf_number){
  for(i in 1:length(viaf_number)){
  print(paste0("Book ", i, " ", Sys.time()))  
  }
  base_url <- "http://viaf.org/viaf/"
  # navigate to the correct url
  remDr$navigate(paste0(base_url, viaf_number))
  # scrape the gender for that author
  gender <- read_html(remDr$getPageSource()[[1]])%>%
    html_nodes(".langHi0+ .langHi0")%>%
    html_text()
  
  gender
  
}

get_all_genders <- function(url_df){
  # scrape all viaf numbers and add them to input df
  # function is expecting a df with encoded urls
  url_df$viaf <- map_chr(url_df$urls, get_viaf_number)
  
  # use these scraped viafs to return the gender of the author as a new variable to the input df
  url_df$gender <- map_chr(url_df$viaf, get_gender)
  url_df
}

df <- get_all_genders(urls)


for(i in 1:length(test_urls)){
  print(paste0("Book ", i, " ", Sys.time()))  
}
```


