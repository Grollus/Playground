---
title: "Analysis of Beer Advocate Reviews Dataset"
subtitle: "Walkthrough of a data science take-home interview test"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      cache = TRUE,
                      fig.align = 'center',
                      out.width = "100%",
                      out.height = "90%")
library(ggplot2)
theme_set(theme_minimal())
```

```{r data-load, results = FALSE, message = FALSE, include = TRUE}
library(tidyverse)
library(magrittr)
library(scales)
# Read in the raw data
reviews_raw <- read_csv("Data/beerreviews/beer_reviews.csv")
```

Interviewing for data science jobs is hard. Since the job definition and responsibilities vary hugely between
companies and roles, you can never quite know what areas of the field you'll be asked about during an interview.
This is stressful for the interviewee and can result in misleading results for the interviewer. 

A while back I stumbled on a great [blog post](https://www.linkedin.com/pulse/how-hire-test-data-skills-one-size-fits-all-interview-tanya-cashorali/) by Tanya
Cashorali where she goes into great detail about why she's taken to using a take-home 'test' to evaluate potential data scientists.
She even lays out a test dataset and problem set for interviews. 

I loved her take on how bad many data science interviews are and I thought it would be great practice(and maybe useful for anyone who stumbles upon this post!) to work through the test she lays out. The test she proposes is simple, and can be answered on many different
levels depending on the candidate. Using a dataset from the popular beer(yay beer!) review website Beer Advocate, you just need to 
answer four analytics questions and present your findings:

1) Which brewery produces the strongest beers by ABV?
2) If you had to pick 3 beers to recommend using only this data, which would you pick?
3) Which of the factors(aroma, taste, appearance, palette) are most important in determining the overall quality of a beer?
4) If I typically enjoy a beer due to its aroma and appearance, which beer style should I try?

This is a really exciting dataset as a beer lover, so without further ado, let's dive in!


### Data Exploration before Analysis

I always need to get a feel for the dataset before doing any sort of analysis. With `r scales::comma(nrow(reviews_raw))` reviews, the
dataset is large enough to need some exploration to get a handle on, but small enough that I don't have to worry too much about performance or speed while doing so. 

I want to know a bit about who's represented in the dataset. With a little dplyr, I can see that there are `r scales::comma(length(unique(reviews_raw$brewery_id)))` breweries and `r scales::comma(length(unique(reviews_raw$beer_beerid)))` beers represented. The names are pretty familiar if you are a beer drinker. It seems that the beer advocate dataset is a picture of the craft brew beer scene
and not necessarily representative of all beer drinkers. After all, you have to be pretty 'in' to beer to review every beer you've tried. 
The big 3 American macro-breweries (Anheuser, Millers and Coors) are present in the data, but not sitting at the top with the most reviews
like you might expect from the breweries that sell the most beer by [volume](https://www.forbes.com/sites/garystoller/2018/03/20/craft-breweries-dominate-the-top-50-but-guess-which-giants-rule-the-beer-market/#3c5064e6dcad). This isn't particularly important for the questions being asked here, but it's 
good to identify biases in your dataset as you recognize them.

#### Most Reviewed Breweries
```{r, most-reviewed-breweries}
# 5840 different breweries represented in the dataset: Sam Adams the largest
reviews_raw%>%
  count(brewery_id, brewery_name, sort = TRUE)%>%
  select(brewery_name, n)%>%
  head(8)%>%
  formattable::formattable()
```


#### Most Reviewed Beers
```{r, most-reviewed-beers}
#  66055 different beers reviewed. Lots of Stone, Sierra Nevada representation
reviews_raw%>%
  count(beer_beerid, beer_name, brewery_name, sort = TRUE)%>%
  select(beer_name, brewery_name, n)%>%
  head(8)%>%
  formattable::formattable()
```


I am also interested to see how many reviews beers typically get. If I'm going to recommend beers, it's probably not a great idea
to recommend a beer that's only been reviewed 1 time--even if it got a perfect review. Similarly, saying a brewery produces the strongest
beers by ABV probably means two different things if one brewery makes 15 different beers while the other only makes 1. This gets to the issue of thresholding--at what level do I need to exclude reviews/beers/breweries for not having enough data to give a reasonable answer?

#### How Many Reviews do Beers Receive? 

```{r, review-counts}
# create df of how man reviews each unique beer has gotten
reviews_per_beer <- reviews_raw %>%
  count(beer_beerid, beer_name, sort = TRUE)

# create data frame of mean and median for plot and easy labeling
reviews_measures <- reviews_per_beer %>%
  summarise(Mean = round(mean(n)),
            Median = median(n))%>%
  # this gather just makes it a tidy df so I can use ggplot easily and get legend labels
  gather()


reviews_per_beer %>%
  ggplot(aes(n))+
  geom_histogram(fill = 'lightblue', color = 'white')+
  # need log scale to have an interpretable plot--most beers get very few reviews
  scale_x_log10()+
  # use reviews_measures to add mean/median lines to plot
  geom_vline(data = reviews_measures, aes(xintercept = value, color = key), lty = 2)+
  # adjust y axis limits for 'better' looking graph
  scale_y_continuous(limits = c(0,25000), expand = c(0,0))+
  labs(color = "")+
  theme(legend.position = c(.75, .87))+
  labs(x = "# of Reviews",
       y = "# of beers",
       title = "Number of reviews received by beers",
       subtitle = "~80% of beers receive 10 or fewer reviews")
  
```

It turns out that most beers get *very* few reviews. Median reviews for a beer is just `r median(reviews_per_beer$n)`, but the mean,
`r round(mean(reviews_per_beer$n))`, is heavily pushed to the right by the very popular beers in the right tail of the distribution.

#### How Many Beers do Breweries Produce?


```{r beer-produced-by-breweries}
# removed beers without abv listings and then see how many different beers
# breweries are producing. 
# Also tried to see how that compared to the mean/median # of beers made, but
# that doesn't seem particularly useful
beers_per_brewery <- reviews_raw %>%
  filter(!is.na(beer_abv))%>%
  count(brewery_name, beer_name, sort = TRUE)%>%
  count(brewery_name, sort = TRUE)%>%
  mutate(produce_one = sum(n==1)/n())

brewery_measures <- beers_per_brewery %>%
  summarise(Mean = mean(n),
         Median = median(n))%>%
  gather()

beers_per_brewery %>%
  ggplot(aes(n))+
  geom_histogram(fill = 'lightblue', color = 'white')+
  geom_vline(data = brewery_measures, aes(xintercept = value, color = key), lty = 2)+
  scale_x_log10()+
  scale_y_continuous(limits = c(0, 1100), expand = c(0,0))+
  theme(legend.position = c(.75, .87))+
  labs(x = "# of Beers",
       y = "# of Breweries",
       title = "Number of Unique Beers Produced by Breweries",
       subtitle = paste0(percent(beers_per_brewery$produce_one[1]), " of breweries in the dataset only produce 1 beer"),
       color = "")

```

Over `r scales::percent(beers_per_brewery$produce_one[1])` of breweries only make one beer. The mean number of beers a brewery produces
is just `r scales::number(brewery_measures$value[1])`, with a median of `r scales::number(brewery_measures$value[2])`. So for the most 
part, breweries aren't producing(or at least people aren't reviewing) *that* many different beers. This will need to be dealt with
when deciding how many beers is 'enough' to include a brewery in the highest ABV brewery question.


#### Does the Style of Beer Matter? You Bet it Does!

There are 104 different styles of beer represented in the dataset. As you can see, several styles are **really** popular 
in the dataset, while others are quite uncommon.
```{r beer-styles}
# 104 styles of beer(some seem very close to the same thing)
reviews_raw%>%
  count(beer_style, sort = TRUE)
```

I think it will be useful to know what effect the style of beer has on the overall review score. Knowing a little bit about beer,
I could see a situation where one style--let's say Quadrupels(Quads)--are always really highly scored, but another--maybe American Lagers--
are generally scored very low. Just knowing the style of the beer might give us a lot of information about how that beer is probably
going to be reviewed.


Looking at the top 5 and bottom 5 scoring styles, it's pretty shocking how different their average scores are. Is it really that
difficult to make a delicious Light Lager? Or on the flip side, are Quads that easy to make--or do humans just love the taste of them?
Who knows, but I know I want to dive a tiny bit deeper to see how much variance there is in the reviews of each style of beer.
Maybe it's possible to get a good Light Lager, but there's a whole host of bad ones out there dragging down the average score.

```{r, top-and-bottom-five}
# Looking at how average score varies by style of beer.
avg_score_by_style <- reviews_raw%>%
  group_by(beer_style)%>%
  summarise(avg_score = mean(review_overall),
            reviews = n())%>%
  ungroup()%>%
  arrange(desc(avg_score))

# Grab top 5 and bottom 5 and bind together in df
top_5 <- avg_score_by_style %>%
  top_n(5, avg_score)
bottom_5 <- avg_score_by_style%>%
  top_n(-5, avg_score)
top_bottom_5 <- bind_rows(top_5, bottom_5)

formattable::formattable(top_bottom_5)

```

#### Overall Review Scores for Different Beer Styles: How Confident are you?

I can generate confidence intervals to see how much variance there is in the scores of different styles of beer. There's
a couple different ways to approach this--mainly related to how you decide to group--and each have their advantages so I include both here.


The first approach groups by unique beers and then style. So, for example, beer 1904 is an American IPA that's had 3000 reviews and 
and average score of ~4.17. Once I have this information, I can generate confidence intervals, but I need to deal with the tricky 
question of what beers to include in the calculations. If a beer is only reviewed 1 time, should it be included? What about if a
beer style only has 15 different beers to it's name? In order to get a good estimation of both the point estimate and the confidence
intervals, I need to have enough data about each beer, otherwise it could be that only one person reviewed a beer and they just had
a terrible day and gave it 1 star--and then are skewing our analysis here.

It's definitely a judgement call what level to filter here, but I want to be reasonable confident my intervals are meaningful. 
I'm only keeping beers that have >= 30 reviews(about 7100), then I count how many beers of each style remain and I'm filtering again to 
keep those styles with at least 30 beers--so each style that remains has at least 30 different beers that had at least 30 reviews.
```{r, top-25-styles-conf-int}
library(broom)
# This first approach groups by the unique beers and styles--so it averages
# the score of each individual beer and then our final estimates are an average of 
# all the unique beers in that beer style. This givens a lot of weight to beers with
# few reviews

# Have to group differently to show conf intervals for scores of beer styles
by_beerid <- reviews_raw %>%
  group_by(beer_beerid, beer_style)%>%
  summarise(reviews = n(),
            avg_score = mean(review_overall))%>%
  ungroup()%>%
  arrange(desc(reviews))

# generate confidence intervals for styles of beer
beer_style_conf_int <- by_beerid%>%
  # filter to beers with at least 30 reviews- leaves us with 7174 different beers
  filter(reviews >= 30)%>%
  # count how many beers of each style remain
  add_count(beer_style)%>%
  #filter again to keep styles with at least 30 different beers remaining
  filter(n >= 30)%>%
  # creates nested list column for each beer style with each beer included, it's average score
  # and the total number of beers of that style
  nest(-beer_style)%>%
  # use this list to do a t.test and create a confidence interval for each beer style
  mutate(model = map(data, ~t.test(.$avg_score)))%>%
  # unnest all this information into a nice tidy df
  unnest(map(model, tidy))

beer_style_conf_int%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(25, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  theme(axis.text.y = element_text(size = 6))+
  labs(x = "Overall Review Score",
       y = "",
       title = "Top 25 Beer Styles",
       subtitle = "Estimated Review Score of Beer Styles with 95% Confidence Intervals")

```

The top style here don't surprise me a ton, but it's very interesting to see which styles are more variable than others. If you
were to just pick a random Imperial Stout/Barleywine/American IPA, you can be pretty confident you know exactly how good of a beer
you are going to get. But if you pick something like a Gueuze/Foreign Stout/Old Ale, there's just a bit more variation in how 
'good' the beer is--it could be the best beer you've ever had or it could just be OK.

The bottom part of the style list is interesting not for the styles on it, but for how much more variance there is. Be careful to look
at the axis labels here(since they are a little misleading compared to the top styles graph), because there's actually quite a bit more variance in how good or bad these beers are. Many of these styles have confidence intervals .2-.3 points wide, whereas the top styles most were rarely over .1. This sounds to me like 'bad' beer styles can still produce good beers.

```{r bottom-25-beer-styles}
beer_style_conf_int%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(-25, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  theme(axis.text.y = element_text(size = 6))+
  scale_x_continuous(limits = c(2.35, 3.9), breaks = seq(from = 2.4, to = 4, by = .1), expand = c(0,0))+
  labs(x = "Overall Review Score",
       y = "",
       title = "Bottom 25 Beer Styles",
       subtitle = "Estimated Review Score of Beer Styles with 95% Confidence Intervals")

```

```{r top-style-all-beers-approach}
# This approach creates a unique id for each review(since there isn't one in the dataset)
# and then it groups by beer style. This means I keep all individual reviews and those
# unique beers with very few reviews don't get as much weight. This seems like a 
# really clunky way of doing this, so I am probably missing a much simpler way.

#Assign a unique identifier to each review first
by_reviewid <- reviews_raw %>%
  mutate(unique_id = 1:nrow(reviews_raw))%>%
  mutate(avg_score = review_overall)%>%
  select(unique_id, beer_style, avg_score, beer_beerid)

by_reviewid %>%
  count(beer_style)%>%
  ggplot(aes(n))+
  geom_histogram(fill = 'lightblue', color = 'white')+
  scale_x_log10()
  

# generate the conf intervals on the reviewid level data
beer_style_conf_int_by_reviewid <- by_reviewid%>%
  add_count(beer_style)%>%
  # filters to beers styles with greater than 2500 reviews
  filter(n >=1000)%>%
  nest(-beer_style)%>%
  mutate(model = map(data, ~t.test(.$avg_score)))%>%
  unnest(map(model, tidy))


  

# plot
beer_style_conf_int_by_reviewid%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(25, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  theme(axis.text.y = element_text(size = 6))+
  labs(x = "Overall Review Score",
       y = "",
       title = "Average review score of different styles of beer",
       subtitle = "Averaged from individual reviews of beer styles with at least 2500 total reviews")
```

```{r}
beer_style_conf_int_by_reviewid%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(-25, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  theme(axis.text.y = element_text(size = 6))+
  labs(x = "Overall Review Score",
       y = "",
       title = "Average review score of different styles of beer",
       subtitle = "Averaged from individual reviews of beer styles with at least 2500 total reviews")
```


```{r}
# Looking at beer abv; removing reviews with missing abv(about 67k)
reviews_raw %>%
  filter(!is.na(beer_abv))%>%
  count(beer_abv, sort = TRUE)

reviews_raw %>%
  filter(!is.na(beer_abv))%>%
  ggplot(aes(beer_abv))+
  geom_histogram()

# Only 18 beers in the dataset are about 20% abv
reviews_raw%>%
  filter(!is.na(beer_abv), beer_abv > 20)%>%
  count(beer_name, beer_abv, brewery_name, beer_style, sort = TRUE)

reviews_raw%>%
  filter(!is.na(beer_abv), beer_abv > 20)%>%
  ggplot(aes(beer_abv))+
  geom_histogram()

# Most beers hoover around the 5% range which isn't surprising as a beer drinker
reviews_raw %>%
  filter(!is.na(beer_abv), beer_abv < 20)%>%
  ggplot(aes(beer_abv))+
  geom_histogram()
```


### Which brewery produces the strongest beers by ABV%?
This question has more nuance than it appears at first glance. After exploring the 
data, it is clear that some breweries produce some super high abv beers, but very
few people ever try them. Or, a small brewery will produce a very small number of beers
and they will all be very high abv--so perhaps they are specializing in high abv beers.
The question then becomes what are we trying to answer with the question 'which brewery
produces the strongest beers by ABV'?

Ways to approach this:
1. Simple average of abv of all beers produced by a brewery.
2. Threshold breweries to breweries producing > than X number of different
beers. Then average those abvs.
3. Use median to ameliorate the effect of a brewery having just a handful of
really high abv beers skewing their results.
4. Is average(or median) even a good measure of this? It's possible a brewery produces
a lot of high abv beers, but they also produce a ton of more standard fare. Isn't that brewery
a good choice for high abv lovers?
```{r}
average_abv <- reviews_raw %>%
  # remove reviews with missing abv
  filter(!is.na(beer_abv), !is.na(brewery_name))%>%
  # don't want a weighted average, so we grab a df of unique beers for all breweries
  distinct(brewery_id, beer_name, .keep_all = TRUE)%>%
  # group them by the brewery and the beer
  group_by(brewery_id, brewery_name)%>%
  # just doing a simple average here(very debatable) and grabbing the # of unique beers produced by brewery
  summarise(avg_abv = mean(beer_abv),
            med_abv = median(beer_abv),
            num_beers = n()) %>%
  ungroup()%>%
  # optional filtering step here to weed out breweries only producing a small number of beers
  # filter(num_beers >=10)%>%
  mutate(mean_beers_produced_overall = mean(num_beers))
 

# several brewery_names aren't unique; append the unique id to those so identify
duplicate_names <- average_abv %>%
  filter(duplicated(.[["brewery_name"]]))%>%
  select(brewery_name)

average_abv <- average_abv %>%
  mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
                               brewery_name))

average_abv %>%
  mutate(brewery_name = fct_reorder(brewery_name, avg_abv))%>%
  filter(num_beers >= 7)%>%
  top_n(25, avg_abv)%>%
  ggplot(aes(avg_abv, brewery_name))+
  geom_point()+
  labs(x = "%ABV",
       y = "", 
       title = "Top 25 Breweries producing the highest average abv beers",
       subtitle = "Only Breweries producing 7 or more different beers")

average_abv %>%
  distinct(brewery_name, .keep_all = TRUE)%>%
  mutate(brewery_name = fct_reorder(brewery_name, med_abv))%>%
  filter(num_beers >= 7)%>%
  top_n(25, med_abv)%>%
  ggplot(aes(med_abv, brewery_name))+
  geom_point()+
  labs(x = "%ABV",
       y = "", 
       title = "Top 25 Breweries producing the highest median abv beers",
       subtitle = "Only Breweries producing 7 or more different beers")
```

Shorschbrau brewery seems to be head and shoulders above the others in average and median
abv of the beers they produce. With a little googling, it seems they are well known
for pushing the limits of strong beers. They even produce one that is 57% abv!
```{r}
reviews_raw%>%
  filter(!is.na(beer_abv), brewery_name == "Schorschbräu")%>%
  distinct(beer_name, .keep_all = TRUE)%>%
  select(beer_name, beer_abv)
```

Also want to look at those beers above, say 10% abv. Is there some brewery that produces a lot of
those, but gets diluted because they produce so many other 'normal' beers--something like Sam
Adams perhaps.

```{r}
beers_above_ten_perc <- reviews_raw %>%
  filter(!is.na(beer_abv), !is.na(brewery_name))%>%
  #alter the brewery name again like above to uniquely identify those duplicate brewery names
  mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
                               brewery_name))%>%
  distinct(brewery_name,
           beer_name,
           .keep_all = TRUE)%>%
  group_by(brewery_name)%>%
  summarise(beers = n(),
            beers_above_10 = sum(beer_abv >=10),
            per_abv_10 = beers_above_10/beers)

beers_above_ten_perc%>%
  filter(beers >= 20)%>%
  mutate(brewery_name = fct_reorder(brewery_name, per_abv_10))%>%
  top_n(25, per_abv_10)%>%
  ggplot(aes(per_abv_10, brewery_name))+
  geom_point()+
  scale_x_continuous(labels = percent_format())+
  expand_limits(x = .3)+
  labs(x = "Percentage of Beers Above 10% ABV",
       y = "",
       title = "Percentage of Breweries Beers which are greater than 10% ABV",
       subtitle = "For breweries producing more than 20 different beers")



# beers_above_fifteen_perc <- reviews_raw %>%
#   filter(!is.na(beer_abv), !is.na(brewery_name))%>%
#   #alter the brewery name again like above to uniquely identify those duplicate brewery names
#   mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
#                                brewery_name))%>%
#   distinct(brewery_name,
#            beer_name,
#            .keep_all = TRUE)%>%
#   group_by(brewery_name)%>%
#   summarise(beers = n(),
#             beers_above_15 = sum(beer_abv >=15),
#             per_abv_15= beers_above_15/beers)
# 
# beers_above_fifteen_perc%>%
#   filter(beers >= 15)%>%
#   mutate(brewery_name = fct_reorder(brewery_name, per_abv_15))%>%
#   top_n(25, per_abv_15)%>%
#   ggplot(aes(per_abv_15, brewery_name))+
#   geom_point()+
#   scale_x_continuous(labels = percent_format())+
#   expand_limits(x = .02)+
#   labs(x = "Percentage of Beers Above 15% ABV",
#        y = "",
#        title = "Percentage of Breweries Beers which are greater than 15% ABV",
#        subtitle = "For breweries producing more than 15 different beers")
```

If we are looking for breweries that produce a lot of high abv beers in general, 
AleSmith Brewery Company is a good choice with over 70% of the beers they produce
being 10% ABV or higher.


###If you had to pick 3 beers to recommend using only this data, which would you pick?

1. A simple way to do a recommendation is to look at reviewers(and beers) that had a good amount
of reviews. From those you could pick the 3 highest rated beers (by overall rating?)
2. A recommender system based on matrix factorization

```{r}
reviews_raw %>%
  count(review_profilename, sort = TRUE)%>%
  ggplot(aes(n))+
  geom_histogram()+
  scale_x_log10()

# filter to reviewers who have reviewed above 50 beers and beers with greater than 100 reviews
over_50_reviews <- reviews_raw%>%
  add_count(review_profilename)%>%
  filter(n >= 50)%>%
  add_count(beer_beerid)%>%
  rename(num_reviews = n)%>%
  filter(num_reviews >=100, 
         !is.na(beer_name),
         !is.na(review_profilename),
         !is.na(review_overall))%>%
  # add new user_id(starting at 1) and beer_id(starting at 1)
  mutate(., user_id = group_indices(., review_profilename),
         beer_id = group_indices(., beer_beerid))


user_beer_triplet <- over_50_reviews %>%
  select(user_id, beer_id, review_overall)

library(recosystem)
r <- Reco()
train_set = data_memory(user_index = user_beer_triplet$user_id,
                        item_index = user_beer_triplet$beer_id,
                        rating = user_beer_triplet$review_overall,
                        index1 = TRUE)
# dim parameter controls the number of latent factors in the model.
# Default dim = 10 and that's too few--you end up having pretty much
# the same beers recommended to everyone. There is a balancing to be done here
# of creating too many latent factors and essentially overfitting and too few
# and giving really generic recommendations
r$train(train_set, opts = list(dim = 50,                        
                               costp_l1 = 0, costp_l2 = 0.01,   
                               costq_l1 = 0, costq_l2 = 0.01,   
                               niter = 20,                      
                               nthread = 6))


top_three_beers_for_user <- function(user_beer_sparse = user_beer_triplet, user_id = NULL){
  
  # for this proof of concept implementation, just grabbing a random user_id,
  # but I want to be able to grab a specific user too which is what the if statement
  # is for.
  if (is.null(user_id)){
    user <- sample.int(n_distinct(user_beer_sparse$beer_id), 1)
  } else{
    stopifnot(is.numeric(user_id))
    stopifnot(user_id != 0)
    user <- user_id
  }
  
  # grab the number of unique beers to build grid for prediction
  beer <- 1:n_distinct(user_beer_sparse$beer_id)
  
  #build grid of beers to predict for user
  # this doesn't filter out beers they already have reviewed. it predicts them 
  # anyway.This predict can be used later to gauge the accuracy/usefulness of
  # the recommender.
  
  pred <- expand.grid(user = user, beer = beer)
  # build recosystem object used for prediction
  prediction_set <- data_memory(pred$user, 
                                pred$beer, 
                                index1 = TRUE)
  # do the actual predictions and add to prediction grid
  pred$rating <- r$predict(prediction_set, out_memory())
  
  # df of beers the specific user has reviewed
  users_beers <- user_beer_sparse %>%
    filter(user_id %in% user)
  
  # return top three beers by predicted rating
  # remove predictions for beers user already reviewed
  pred %>%
    filter(!(beer %in% users_beers$beer_id))%>%
    arrange(desc(rating))%>%
    # this join allows me to get the beer name, style and brewery name
    # the prediction df is just id numbers and a rating prediction
    inner_join((over_50_reviews%>%
                 distinct(beer_id, .keep_all = TRUE)%>%
                  select(beer_id, beer_name, beer_style, brewery_name)), by = c("beer" = "beer_id"))%>%
    top_n(3, rating)
}

users_top_rated_beers <- function(df = over_50_reviews, user = NULL){
  # function takes a specific user_id and returns the style, name and rating of their top 10 beers
  df%>%
    filter(user_id == user)%>%
    select(user_id, beer_id, beer_name, beer_style, review_overall)%>%
    top_n(10, review_overall)%>%
    arrange(desc(review_overall))
}

```



### Which of the factors(aroma, taste, appearance, palette) are most important 
in determining the overall quality of the beer?

Off the top of my head I can think of two ways to approach this:
1. I can look at the correlation of the 4 sub factors with the overall review
score of a beer.
2. I can build a linear model to see which factors are significant when predicting 
the overall quality of a beer.
  2a. This could be problematic: Think Bayesian Richard McElrith
  


```{r}
library(corrr)
reviews_raw %>%
  select(review_appearance, review_aroma, review_palate, review_taste, review_overall)%>%
  correlate()
```


```{r}
m1 <- lm(review_overall~ review_appearance + review_aroma + review_taste + review_palate, data = reviews_raw)
summary(m1)

m2 <- lm(review_overall ~ review_appearance, data = reviews_raw)
summary(m2)
m3 <- lm(review_overall ~ review_aroma, data = reviews_raw)
summary(m3)
m4 <- lm(review_overall ~ review_taste, data = reviews_raw)
summary(m4)
m5 <- lm(review_overall ~ review_palate, data = reviews_raw)
summary(m5)


```






### If I enjoy a beer due to its aroma and appearance, which beer style should I try?
Again, there are several ways to approach this question.
1. Look at which beers styles have high aroma and appearance reviews and recommend those.
2. Look at aroma and appearance above some threshold value and then recommend the most
highly rated beer style overall amongst those high aroma and appearance scores.



```{r}
aroma_and_appearance <- reviews_raw %>%
  filter(!is.na(brewery_name))%>%
  group_by(beer_style)%>%
  summarise(reviews = n(),
            avg_aroma = mean(review_aroma),
            avg_appearance = mean(review_appearance),
            avg_overall = mean(review_overall))%>%
  ungroup()%>%
  mutate(total_aroma_appear = avg_aroma + avg_appearance,
         avg_aroma_appear = (avg_aroma + avg_appearance)/2)%>%
  arrange(desc(avg_aroma_appear), desc(avg_overall))


aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_aroma_appear))%>%
  top_n(25, avg_aroma_appear)%>%
  ggplot(aes(avg_aroma_appear, beer_style))+
  geom_point()+
  geom_point(aes(x = avg_overall), color = 'red', alpha = .3)+
  labs(x = "Average of Aroma and Appearance Scores",
       y = "")

aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_aroma))%>%
  top_n(25, avg_aroma)%>%
  ggplot(aes(avg_aroma, beer_style))+
  geom_point()+
  labs(x = "Average of Aroma Score",
       y = "")

aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_appearance))%>%
  top_n(25, avg_appearance)%>%
  ggplot(aes(avg_appearance, beer_style))+
  geom_point()+
  labs(x = "Average of Appearance Score",
       y = "")
```

This plot of style is not too surprising if you are a beer snob. American and Russian 
imperial stouts are both highly fragrant, richly colored beers--which happen to be delicious. And a Quad
is similarly known for it's aroma and beautiful color. This seems to be a good representation of
fragrant beers with rich, pleasing appearances in the glass.


#### Code Appendix
This section includes additional code that was used--mainly to explore the data--but that didn't make it past the cutting stage of
report writing. It's included because it's interesting, if not directly relevant to the analysis. 
```{r, code-appendix}
# A look at how often beers from the 'macro' breweries are reviewed
reviews_raw%>%
  filter(brewery_name %in% c("Anheuser-Busch", "Coors Brewing Company", "Miller Brewing Co.", "Heineken Nederland B.V."))%>%
  count(brewery_id, brewery_name, sort = TRUE)

# Plotting distribution of 5 different review categories
reviews_raw %>%
  select(starts_with('review'), -review_profilename)%>%
  gather(review_type, score, -review_time)%>%
  ggplot(aes(score))+
  geom_histogram()+
  facet_wrap(~review_type)+
  scale_y_continuous(labels = number_format(), limits = c(0, 700000), expand = c(0, 0))+
  labs(y = "",
       title = "Distribution of Review Scores",
       subtitle = "Scores all have similar shapes, though Appearance may be a tad narrower")
```

