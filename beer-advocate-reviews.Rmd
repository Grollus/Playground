---
title: "Analysis of Beer Advocate Reviews Dataset"
subtitle: "Walkthrough of a data science take-home interview test"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      message = FALSE,
                      warning = FALSE,
                      cache = TRUE,
                      fig.align = 'center',
                      out.width = "100%",
                      out.height = "90%")
library(ggplot2)
theme_set(theme_minimal())
```

```{r data-load, results = FALSE, message = FALSE, include = TRUE}
library(tidyverse)
library(magrittr)
library(scales)
# Read in the raw data
reviews_raw <- read_csv("Data/beerreviews/beer_reviews.csv")
```

Interviewing for data science jobs is hard. Since the job definition and responsibilities vary hugely between
companies and roles, you can never quite know what areas of the field you'll be asked about during an interview.
This is stressful for the interviewee and can result in misleading results for the interviewer. 

A while back I stumbled on a great [blog post](https://www.linkedin.com/pulse/how-hire-test-data-skills-one-size-fits-all-interview-tanya-cashorali/) by Tanya
Cashorali where she goes into great detail about why she's taken to using a take-home 'test' to evaluate potential data scientists.
She even lays out a test dataset and problem set for interviews. 

I loved her take on how bad many data science interviews are and I thought it would be great practice(and maybe useful for anyone who stumbles upon this post!) to work through the test she lays out. The test she proposes is simple, and can be answered on many different
levels depending on the candidate. Using a dataset from the popular beer(yay beer!) review website Beer Advocate, you just need to 
answer four analytics questions and present your findings:

1) Which brewery produces the strongest beers by ABV?
2) If you had to pick 3 beers to recommend using only this data, which would you pick?
3) Which of the factors(aroma, taste, appearance, palette) are most important in determining the overall quality of a beer?
4) If I typically enjoy a beer due to its aroma and appearance, which beer style should I try?

This is a really exciting dataset as a beer lover, so without further ado, let's dive in!


## Data Exploration before Analysis

I always need to get a feel for the dataset before doing any sort of analysis. With `r scales::comma(nrow(reviews_raw))` reviews, the
dataset is large enough to need some exploration to get a handle on, but small enough that I don't have to worry too much about performance or speed while doing so. 

I want to know a bit about who's represented in the dataset. With a little dplyr, I can see that there are `r scales::comma(length(unique(reviews_raw$brewery_id)))` breweries and `r scales::comma(length(unique(reviews_raw$beer_beerid)))` beers represented. The names are pretty familiar if you are a beer drinker. It seems that the beer advocate dataset is a picture of the craft brew beer scene
and not necessarily representative of all beer drinkers. After all, you have to be pretty 'in' to beer to review every beer you've tried. 
The big 3 American macro-breweries (Anheuser, Millers and Coors) are present in the data, but not sitting at the top with the most reviews
like you might expect from the breweries that sell the most beer by [volume](https://www.forbes.com/sites/garystoller/2018/03/20/craft-breweries-dominate-the-top-50-but-guess-which-giants-rule-the-beer-market/#3c5064e6dcad). This isn't particularly important for the questions being asked here, but it's 
good to identify biases in your dataset as you recognize them.

### Most Reviewed Breweries
```{r, most-reviewed-breweries}
# 5840 different breweries represented in the dataset: Sam Adams the largest
reviews_raw%>%
  count(brewery_id, brewery_name, sort = TRUE)%>%
  select(brewery_name, n)%>%
  head(8)%>%
  formattable::formattable()
```


### Most Reviewed Beers
```{r, most-reviewed-beers}
#  66055 different beers reviewed. Lots of Stone, Sierra Nevada representation
reviews_raw%>%
  count(beer_beerid, beer_name, brewery_name, sort = TRUE)%>%
  select(beer_name, brewery_name, n)%>%
  head(8)%>%
  formattable::formattable()
```


I am also interested to see how many reviews beers typically get. If I'm going to recommend beers, it's probably not a great idea
to recommend a beer that's only been reviewed 1 time--even if it got a perfect review. Similarly, saying a brewery produces the strongest
beers by ABV probably means two different things if one brewery makes 15 different beers while the other only makes 1. This gets to the issue of thresholding--at what level do I need to exclude reviews/beers/breweries for not having enough data to give a reasonable answer?

### How Many Reviews do Beers Receive? 

```{r, review-counts}
# create df of how man reviews each unique beer has gotten
reviews_per_beer <- reviews_raw %>%
  count(beer_beerid, beer_name, sort = TRUE)

# create data frame of mean and median for plot and easy labeling
reviews_measures <- reviews_per_beer %>%
  summarise(Mean = round(mean(n)),
            Median = median(n))%>%
  # this gather just makes it a tidy df so I can use ggplot easily and get legend labels
  gather()


reviews_per_beer %>%
  ggplot(aes(n))+
  geom_histogram(fill = 'lightblue', color = 'white')+
  # need log scale to have an interpretable plot--most beers get very few reviews
  scale_x_log10()+
  # use reviews_measures to add mean/median lines to plot
  geom_vline(data = reviews_measures, aes(xintercept = value, color = key), lty = 2)+
  # adjust y axis limits for 'better' looking graph
  scale_y_continuous(limits = c(0,25000), expand = c(0,0))+
  labs(color = "")+
  theme(legend.position = c(.75, .87))+
  labs(x = "# of Reviews",
       y = "# of beers",
       title = "Number of reviews received by beers",
       subtitle = "~80% of beers receive 10 or fewer reviews")
  
```

It turns out that most beers get *very* few reviews. Median reviews for a beer is just `r median(reviews_per_beer$n)`, but the mean,
`r round(mean(reviews_per_beer$n))`, is heavily pushed to the right by the very popular beers in the right tail of the distribution.

### How Many Beers do Breweries Produce?


```{r beer-produced-by-breweries}
# removed beers without abv listings and then see how many different beers
# breweries are producing. 
# Also tried to see how that compared to the mean/median # of beers made, but
# that doesn't seem particularly useful
beers_per_brewery <- reviews_raw %>%
  filter(!is.na(beer_abv))%>%
  count(brewery_name, beer_name, sort = TRUE)%>%
  count(brewery_name, sort = TRUE)%>%
  mutate(produce_one = sum(n==1)/n())

brewery_measures <- beers_per_brewery %>%
  summarise(Mean = mean(n),
         Median = median(n))%>%
  gather()

beers_per_brewery %>%
  ggplot(aes(n))+
  geom_histogram(fill = 'lightblue', color = 'white')+
  geom_vline(data = brewery_measures, aes(xintercept = value, color = key), lty = 2)+
  scale_x_log10()+
  scale_y_continuous(limits = c(0, 1100), expand = c(0,0))+
  theme(legend.position = c(.75, .87))+
  labs(x = "# of Beers",
       y = "# of Breweries",
       title = "Number of Unique Beers Produced by Breweries",
       subtitle = paste0(percent(beers_per_brewery$produce_one[1]), " of breweries in the dataset only produce 1 beer"),
       color = "")

```

Over `r scales::percent(beers_per_brewery$produce_one[1])` of breweries only make one beer. The mean number of beers a brewery produces
is just `r scales::number(brewery_measures$value[1])`, with a median of `r scales::number(brewery_measures$value[2])`. So for the most 
part, breweries aren't producing(or at least people aren't reviewing) *that* many different beers. This will need to be dealt with
when deciding how many beers is 'enough' to include a brewery in the highest ABV brewery question.


### Does the Style of Beer Matter? You Bet it Does!

There are 104 different styles of beer represented in the dataset. As you can see, several styles are **really** popular 
in the dataset, while others are quite uncommon.
```{r beer-styles}
# 104 styles of beer(some seem very close to the same thing)
reviews_raw%>%
  count(beer_style, sort = TRUE)
```

I think it will be useful to know what effect the style of beer has on the overall review score. Knowing a little bit about beer,
I could see a situation where one style--let's say Quadrupels(Quads)--are always really highly scored, but another--maybe American Lagers--
are generally scored very low. Just knowing the style of the beer might give us a lot of information about how that beer is probably
going to be reviewed.


Looking at the top 5 and bottom 5 scoring styles, it's pretty shocking how different their average scores are. Is it really that
difficult to make a delicious Light Lager? Or on the flip side, are Quads that easy to make--or do humans just love the taste of them?
Who knows, but I know I want to dive a tiny bit deeper to see how much variance there is in the reviews of each style of beer.
Maybe it's possible to get a good Light Lager, but there's a whole host of bad ones out there dragging down the average score.

```{r, top-and-bottom-five}
# Looking at how average score varies by style of beer.
avg_score_by_style <- reviews_raw%>%
  group_by(beer_style)%>%
  summarise(avg_score = mean(review_overall),
            reviews = n())%>%
  ungroup()%>%
  arrange(desc(avg_score))

# Grab top 5 and bottom 5 and bind together in df
top_5 <- avg_score_by_style %>%
  top_n(5, avg_score)
bottom_5 <- avg_score_by_style%>%
  top_n(-5, avg_score)
top_bottom_5 <- bind_rows(top_5, bottom_5)

formattable::formattable(top_bottom_5)

```

### Overall review scores for different beer styles: How confident are you?

I can generate confidence intervals to see how much variance there is in the scores of different styles of beer. There are
a couple different ways to approach this--mainly related to how you decide to group--and each have their advantages so I include both here.

#### Filter, then filter again!  Styles that have 30 different beers that had at least 30 reviews

The first approach groups by unique beers and then style. So, for example, beer 1904 is an American IPA that's had 3000 reviews and 
and average score of ~4.17. Once I have this information, I can generate confidence intervals, but I need to deal with the tricky 
question of what beers to include in the calculations. If a beer is only reviewed 1 time, should it be included? What about if a
beer style only has 15 different beers to it's name? In order to get a good estimation of both the point estimate and the confidence
intervals, I need to have enough data about each beer, otherwise it could be that only one person reviewed a beer and they just had
a terrible day and unjustly gave it 1 star--and then are skewing our analysis here.

It's definitely a judgement call what level to filter here, but I want to be reasonably confident my intervals are meaningful. 
I'm only keeping beers that have >= 30 reviews(about 7100), then I count how many beers of each style remain and I'm filtering again to 
keep those styles with at least 30 beers--so each style that remains has at least 30 different beers that had at least 30 reviews.
```{r, top-25-styles-conf-int}
library(broom)
# This first approach groups by the unique beers and styles--so it averages
# the score of each individual beer and then our final estimates are an average of 
# all the unique beers in that beer style. This givens a lot of weight to beers with
# few reviews

# Have to group differently to show conf intervals for scores of beer styles
by_beerid <- reviews_raw %>%
  group_by(beer_beerid, beer_style)%>%
  summarise(reviews = n(),
            avg_score = mean(review_overall))%>%
  ungroup()%>%
  arrange(desc(reviews))

#quick df to use for getting number of beers remaining after filter(awful way to do this)
filtered_beers <- by_beerid%>%
  # filter to beers with at least 30 reviews- leaves us with 7174 different beers
  filter(reviews >= 30)%>%
  # count how many beers of each style remain
  add_count(beer_style)%>%
  #filter again to keep styles with at least 30 different beers remaining
  filter(n >= 30)

# generate confidence intervals for styles of beer
beer_style_conf_int <- by_beerid%>%
  # filter to beers with at least 30 reviews- leaves us with 7174 different beers
  filter(reviews >= 30)%>%
  # count how many beers of each style remain
  add_count(beer_style)%>%
  #filter again to keep styles with at least 30 different beers remaining
  filter(n >= 30)%>%
  # creates nested list column for each beer style with each beer included, it's average score
  # and the total number of beers of that style
  nest(-beer_style)%>%
  # use this list to do a t.test and create a confidence interval for each beer style
  mutate(model = map(data, ~t.test(.$avg_score)))%>%
  # unnest all this information into a nice tidy df
  unnest(map(model, tidy))

beer_style_conf_int%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(20, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  theme(axis.text.y = element_text(size = 6))+
  labs(x = "Overall Review Score",
       y = "",
       title = "Top 20 Beer Styles",
       subtitle = "Estimated Review Score of Beer Styles with 95% Confidence Intervals")

```

The top styles here don't surprise me a ton, but it's very interesting to see which styles are more variable than others. If you
were to just pick a random Imperial Stout/Barleywine/American IPA, you can be pretty confident you know exactly how good of a beer
you are going to get. But if you pick something like a Gueuze/Foreign Stout/Old Ale, there's just a bit more variation in how 
'good' the beer is--it could be the best beer you've ever had or it could just be OK.

The bottom part of the style list is interesting not for the styles on it, but for how much more variance there is. Be careful to look
at the axis labels here(since they are a little misleading compared to the top styles graph), because there's actually quite a bit more variance in how good or bad these beers are. Many of these styles have confidence intervals .2-.3 points wide, whereas the top styles most were rarely over .1. English Strong Ales, for example, have a 95% confidence interval from `r round(beer_style_conf_int[beer_style_conf_int$beer_style == "English Strong Ale",]$conf.low, 2)` - `r round(beer_style_conf_int[beer_style_conf_int$beer_style == "English Strong Ale",]$conf.high, 2)`. This means it's possible that a English Strong Ale is rated as highly as a beer on the top 20 beer styles graph. But it also means that there are a lot of English Strong Ales that just aren't that good. Simply put, there's a lot of variance in how good the style is.

```{r bottom-25-beer-styles}
beer_style_conf_int%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(-20, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  theme(axis.text.y = element_text(size = 6))+
  scale_x_continuous(limits = c(2.35, 3.9), breaks = seq(from = 2.4, to = 4, by = .1), expand = c(0,0))+
  labs(x = "Overall Review Score",
       y = "",
       title = "Bottom 20 Beer Styles",
       subtitle = "Estimated Review Score of Beer Styles with 95% Confidence Intervals")

```

#### Keep all the beers--within reason!

The filtering approach above has a clear bias. Beers--and beer styles-- that are popular are included in the final results. There's a very good reason for filtering out those beers, but we definitely lose a lot of beers in the process--some `r scales::comma(nrow(by_beerid) - nrow(filtered_beers))`. Another approach that avoids losing so many beers is to avoid filtering out individual beers with very few reviews and instead to treat beers as members of the beer style family and not remember individual beers just because they only have a handful of reviews. I do a quick filter to make sure no style has fewer than 30 reviews(spoiler: none do!) and then plot the results with confidence intervals.

```{r top-styles-all-beers-approach}
# This approach creates a unique id for each review(since there isn't one in the dataset)
# and then it groups by beer style. This means I keep all individual reviews and those
# unique beers with very few reviews don't get as much weight. This seems like a 
# really clunky way of doing this, so I am probably missing a much simpler way.

#Assign a unique identifier to each review first
by_reviewid <- reviews_raw %>%
  mutate(unique_id = 1:nrow(reviews_raw))%>%
  mutate(avg_score = review_overall)%>%
  select(unique_id, beer_style, avg_score, beer_beerid)

# generate the conf intervals on the reviewid level data
beer_style_conf_int_by_reviewid <- by_reviewid%>%
  add_count(beer_style)%>%
  # filters to beers styles with at least 30 reviews
  filter(n >=30)%>%
  nest(-beer_style)%>%
  mutate(model = map(data, ~t.test(.$avg_score)))%>%
  unnest(map(model, tidy))


# plot
beer_style_conf_int_by_reviewid%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(20, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  theme(axis.text.y = element_text(size = 6))+
  labs(x = "Overall Review Score",
       y = "",
       title = "Top 20 Beer Styles: No filtering to remove beers with few reviews",
       subtitle = "Estimated Review Score of Beer Styles with 95% Confidence Intervals")
```
The results are pretty similar to the other approach with two major differences. First, the confidence intervals are narrower for both graphs. This is expected because of how I did the grouping. Instead of having, say, 100 beers to base the estimates off of, these graphs may use thousands of reviews--remember all those one review beers we filtered out before? With all that additional data, I expected the confidence intervals to narrow. Second, we see a handful of new styles present on each graph. Lambic - Unblended, Gose and Roggenbier all didn't make the cut before, but are present now. 

Essentially though, the information present is pretty similar. Reviewers seem to love stouts, belgian ales, sours and IPAs and they tend to dislike lagers and light beers.

```{r, bottom-styles-all-beers-approach}
beer_style_conf_int_by_reviewid%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(-20, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  theme(axis.text.y = element_text(size = 6))+
  labs(x = "Overall Review Score",
       y = "",
       title = "Bottom 20 Beer Styles: No filtering to remove beers with few reviews",
       subtitle = "Estimated Review Score of Beer Styles with 95% Confidence Intervals")
```




### Which brewery produces the strongest beers by ABV%?

First things first, there's `r scales::comma(sum(is.na(reviews_raw$beer_abv)))` reviews with no ABV data so we need to remove that data. It's only `r scales::percent(sum(is.na(reviews_raw$beer_abv))/nrow(reviews_raw))` of all reviews, so for the time being I'm not going to worry about it too much. You can see from the histogram of the distribution of ABV's that the vast majority of beers are under 10% ABV.
Only 18 beers out of over 66,000! are over 20% ABV.
```{r, beer-abv-exploration}
# Looking at beer abv; removing reviews with missing abv(about 67k)
reviews_raw %>%
  filter(!is.na(beer_abv))%>%
  ggplot(aes(beer_abv))+
  geom_histogram(fill = 'lightblue', color = 'white')+
  scale_y_continuous(labels = scales::comma_format(), expand = c(0,0))+
  scale_x_continuous(labels = scales::number_format(suffix = "%"),
                     expand = c(0,0),
                     breaks = seq(from = 0, to = 60, by = 10))+
  labs(x = "ABV",
       y = "",
       title = "Distribution of Beer ABV %")

# Only 18 beers in the dataset are about 20% abv
reviews_raw%>%
  filter(!is.na(beer_abv), beer_abv > 20)%>%
  count(beer_name, beer_abv, brewery_name, beer_style, sort = TRUE)%>%
  select(-n)%>%
  arrange(desc(beer_abv))%>%
  formattable::formattable()
```

When we divide the data into reviews of beers > 20% ABV and those < 20% ABV, it's easy to see how uncommon reviews for really high ABV beers are. The vast majority of beers review fall right around the 5% range.

```{r, zoom-in-beer-abv-distributions}
reviews_raw%>%
  filter(!is.na(beer_abv), beer_abv > 20)%>%
  ggplot(aes(beer_abv))+
  geom_histogram(fill = 'lightblue', color = 'white')+
  scale_y_continuous(labels = scales::comma_format(), expand = c(0,0))+
  scale_x_continuous(labels = scales::number_format(suffix = "%"))+
  labs(x = "ABV",
       y = "",
       title = "Distribution of Beer ABV: Filtering out Beers below 20% ABV")

# Most beers hoover around the 5% range which isn't surprising as a beer drinker
reviews_raw %>%
  filter(!is.na(beer_abv), beer_abv < 20)%>%
  ggplot(aes(beer_abv))+
  geom_histogram(fill = 'lightblue', color = 'white')+
  scale_y_continuous(labels = scales::comma_format(), expand = c(0,0))+
  scale_x_continuous(labels = scales::number_format(suffix = "%"),
                     expand = c(0,0))+
  labs(x = "ABV",
       y = "",
       title = "Distribution of Beer ABV: Excluding beers above 20% ABV")
```

The question of which brewery produces the strongest beers by ABV has more nuance than it appears at first glance. First, the question asks which brewery produces the strongest **beers** which I am taking to mean it needs to produce more than just one beer that happens to be super strong. Second, how should I measure how strong a breweries beers are? An average of them? Maybe take the median? What about looking at the proportion of beers that a brewery produces that are above some predetermined 'strong' beer threshold? All these methods seem to have merit and are pretty simple and quick. Let's try them out and see what they tell us.

After a little bit of filtering, grouping and aggregation(which you can see in the code fold below), you can see that Schorschbräu ranks at the top of both the mean and median rankings. In fact, the top 4 on both graphs are the same--albeit with one positional swap. One note: Why did I only include breweries that make 4 or more beers? Because the median number of beers produced is 4 and that seemed reasonable.


```{r, strongest-beers-by-mean-median}
average_abv <- reviews_raw %>%
  # remove reviews with missing abv
  filter(!is.na(beer_abv), !is.na(brewery_name))%>%
  # don't want a weighted average, so we grab a df of unique beers for all breweries
  distinct(brewery_id, beer_name, .keep_all = TRUE)%>%
  # group them by the brewery and the beer
  group_by(brewery_id, brewery_name)%>%
  # getting mean, median and number of beers produced by each brewery
  summarise(avg_abv = mean(beer_abv),
            med_abv = median(beer_abv),
            num_beers = n()) %>%
  ungroup()%>%
  # get the median and mean number of beers produced overall--will use for determining the threshold to cutoff breweries
  mutate(mean_beers_produced_overall = mean(num_beers),
         median_beers_produced_overall = median(num_beers))
 

# several brewery_names aren't unique; append the unique id to the brewery name to get a unique brewery_name
duplicate_names <- average_abv %>%
  filter(duplicated(.[["brewery_name"]]))%>%
  select(brewery_name)


average_abv <- average_abv %>%
  mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
                               brewery_name))%>%
  # several breweries have multiple names separated by '/'. Keeping only the first name used for this
  separate(brewery_name, 
           sep = "/",
           into = c("brewery_name", NA))

average_abv %>%
  mutate(brewery_name = fct_reorder(brewery_name, avg_abv))%>%
  filter(num_beers >= 4)%>%
  top_n(10, avg_abv)%>%
  ggplot(aes(avg_abv, brewery_name))+
  geom_point()+
  theme(axis.text.y = element_text(size = 6))+
  scale_x_continuous(labels = scales::number_format(suffix = "%"))+
  labs(x = "ABV",
       y = "", 
       title = "Breweries Producing the 'Strongest' Beers: By Mean",
       subtitle = "Only Breweries producing 4 or more different beers")

average_abv %>%
  distinct(brewery_name, .keep_all = TRUE)%>%
  mutate(brewery_name = fct_reorder(brewery_name, med_abv))%>%
  filter(num_beers >= 4)%>%
  top_n(10, med_abv)%>%
  ggplot(aes(med_abv, brewery_name))+
  geom_point()+
  theme(axis.text.y = element_text(size = 6))+
  scale_x_continuous(labels = scales::number_format(suffix = "%"))+
  labs(x = "ABV",
       y = "", 
       title = "Breweries Producing the 'Strongest' Beers: By Median",
       subtitle = "Only Breweries producing 4 or more different beers")
```


```{r, median-mean-beer-calculation}
mean_median_beerabv <- reviews_raw %>%
  # remove reviews with missing abv
  filter(!is.na(beer_abv), !is.na(brewery_name))%>%
  # don't want a weighted average, so we grab a df of unique beers for all breweries
  distinct(brewery_id, beer_name, .keep_all = TRUE)%>%
  summarise(mean_abv = mean(beer_abv),
            median_abv = median(beer_abv))
```

Now what if the brewery that produces the strongest beers is the one that has the highest proportion of it's beers above some ABV threshold. We know the mean ABV for a beer is `r round(mean_median_beerabv$mean_abv, 2)`% (median `r round(mean_median_beerabv$median_abv, 2)`%)



```{r}

beers_above_ten_perc <- reviews_raw %>%
  filter(!is.na(beer_abv), !is.na(brewery_name))%>%
  #alter the brewery name again like above to uniquely identify those duplicate brewery names
  mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
                               brewery_name))%>%
  separate(brewery_name, 
           sep = "/",
           into = c("brewery_name", NA))%>%
  distinct(brewery_name,
           beer_name,
           .keep_all = TRUE)%>%
  group_by(brewery_name)%>%
  summarise(beers = n(),
            beers_above_10 = sum(beer_abv >=10),
            per_abv_10 = beers_above_10/beers)

beers_above_ten_perc%>%
  filter(beers >= 10)%>%
  mutate(brewery_name = fct_reorder(brewery_name, per_abv_10))%>%
  top_n(25, per_abv_10)%>%
  ggplot(aes(per_abv_10, brewery_name))+
  geom_point()+
  scale_x_continuous(labels = percent_format())+
  expand_limits(x = .3)+
  theme(axis.text.y = element_text(size = 6))+
  labs(x = "Percentage of Beers Above 10% ABV",
       y = "",
       title = "What Percentage of Breweries Beers are > 10% ABV",
       subtitle = "For breweries producing more than 10 different beers")
```

If we are looking for breweries that produce a lot of high abv beers in general, 
AleSmith Brewery Company is a good choice with over 70% of the beers they produce
being 10% ABV or higher.


Shorschbrau brewery seems to be head and shoulders above the others in average and median
abv of the beers they produce. With a little googling, it seems they are well known
for pushing the limits of strong beers. They even produce one that is 57% abv!
```{r}
reviews_raw%>%
  filter(!is.na(beer_abv), brewery_name == "Schorschbräu")%>%
  distinct(beer_name, .keep_all = TRUE)%>%
  select(beer_name, beer_abv)%>%
  arrange(desc(beer_abv))%>%
  formattable::formattable()
```

###If you had to pick 3 beers to recommend using only this data, which would you pick?

1. A simple way to do a recommendation is to look at reviewers(and beers) that had a good amount
of reviews. From those you could pick the 3 highest rated beers (by overall rating?)
2. A recommender system based on matrix factorization

```{r}
reviews_raw %>%
  count(review_profilename, sort = TRUE)%>%
  ggplot(aes(n))+
  geom_histogram()+
  scale_x_log10()

# filter to reviewers who have reviewed above 50 beers and beers with greater than 100 reviews
over_50_reviews <- reviews_raw%>%
  add_count(review_profilename)%>%
  filter(n >= 50)%>%
  add_count(beer_beerid)%>%
  rename(num_reviews = n)%>%
  filter(num_reviews >=100, 
         !is.na(beer_name),
         !is.na(review_profilename),
         !is.na(review_overall))%>%
  # add new user_id(starting at 1) and beer_id(starting at 1)
  mutate(., user_id = group_indices(., review_profilename),
         beer_id = group_indices(., beer_beerid))


user_beer_triplet <- over_50_reviews %>%
  select(user_id, beer_id, review_overall)

library(recosystem)
r <- Reco()
train_set = data_memory(user_index = user_beer_triplet$user_id,
                        item_index = user_beer_triplet$beer_id,
                        rating = user_beer_triplet$review_overall,
                        index1 = TRUE)
# dim parameter controls the number of latent factors in the model.
# Default dim = 10 and that's too few--you end up having pretty much
# the same beers recommended to everyone. There is a balancing to be done here
# of creating too many latent factors and essentially overfitting and too few
# and giving really generic recommendations
r$train(train_set, opts = list(dim = 50,                        
                               costp_l1 = 0, costp_l2 = 0.01,   
                               costq_l1 = 0, costq_l2 = 0.01,   
                               niter = 20,                      
                               nthread = 6))


top_three_beers_for_user <- function(user_beer_sparse = user_beer_triplet, user_id = NULL){
  
  # for this proof of concept implementation, just grabbing a random user_id,
  # but I want to be able to grab a specific user too which is what the if statement
  # is for.
  if (is.null(user_id)){
    user <- sample.int(n_distinct(user_beer_sparse$beer_id), 1)
  } else{
    stopifnot(is.numeric(user_id))
    stopifnot(user_id != 0)
    user <- user_id
  }
  
  # grab the number of unique beers to build grid for prediction
  beer <- 1:n_distinct(user_beer_sparse$beer_id)
  
  #build grid of beers to predict for user
  # this doesn't filter out beers they already have reviewed. it predicts them 
  # anyway.This predict can be used later to gauge the accuracy/usefulness of
  # the recommender.
  
  pred <- expand.grid(user = user, beer = beer)
  # build recosystem object used for prediction
  prediction_set <- data_memory(pred$user, 
                                pred$beer, 
                                index1 = TRUE)
  # do the actual predictions and add to prediction grid
  pred$rating <- r$predict(prediction_set, out_memory())
  
  # df of beers the specific user has reviewed
  users_beers <- user_beer_sparse %>%
    filter(user_id %in% user)
  
  # return top three beers by predicted rating
  # remove predictions for beers user already reviewed
  pred %>%
    filter(!(beer %in% users_beers$beer_id))%>%
    arrange(desc(rating))%>%
    # this join allows me to get the beer name, style and brewery name
    # the prediction df is just id numbers and a rating prediction
    inner_join((over_50_reviews%>%
                 distinct(beer_id, .keep_all = TRUE)%>%
                  select(beer_id, beer_name, beer_style, brewery_name)), by = c("beer" = "beer_id"))%>%
    top_n(3, rating)
}

users_top_rated_beers <- function(df = over_50_reviews, user = NULL){
  # function takes a specific user_id and returns the style, name and rating of their top 10 beers
  df%>%
    filter(user_id == user)%>%
    select(user_id, beer_id, beer_name, beer_style, review_overall)%>%
    top_n(10, review_overall)%>%
    arrange(desc(review_overall))
}

```



### Which of the factors(aroma, taste, appearance, palette) are most important in determining the overall quality of the beer?

Off the top of my head I can think of two ways to approach this:
1. I can look at the correlation of the 4 sub factors with the overall review
score of a beer.
2. I can build a linear model to see which factors are significant when predicting 
the overall quality of a beer.
  2a. This could be problematic: Think Bayesian Richard McElrith
  


```{r}
library(corrr)
reviews_raw %>%
  select(review_appearance, review_aroma, review_palate, review_taste, review_overall)%>%
  correlate()
```


```{r}
m1 <- lm(review_overall~ review_appearance + review_aroma + review_taste + review_palate, data = reviews_raw)
summary(m1)

m2 <- lm(review_overall ~ review_appearance, data = reviews_raw)
summary(m2)
m3 <- lm(review_overall ~ review_aroma, data = reviews_raw)
summary(m3)
m4 <- lm(review_overall ~ review_taste, data = reviews_raw)
summary(m4)
m5 <- lm(review_overall ~ review_palate, data = reviews_raw)
summary(m5)


```






### If I enjoy a beer due to its aroma and appearance, which beer style should I try?
Again, there are several ways to approach this question.
1. Look at which beers styles have high aroma and appearance reviews and recommend those.
2. Look at aroma and appearance above some threshold value and then recommend the most
highly rated beer style overall amongst those high aroma and appearance scores.



```{r}
aroma_and_appearance <- reviews_raw %>%
  filter(!is.na(brewery_name))%>%
  group_by(beer_style)%>%
  summarise(reviews = n(),
            avg_aroma = mean(review_aroma),
            avg_appearance = mean(review_appearance),
            avg_overall = mean(review_overall))%>%
  ungroup()%>%
  mutate(total_aroma_appear = avg_aroma + avg_appearance,
         avg_aroma_appear = (avg_aroma + avg_appearance)/2)%>%
  arrange(desc(avg_aroma_appear), desc(avg_overall))


aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_aroma_appear))%>%
  top_n(25, avg_aroma_appear)%>%
  ggplot(aes(avg_aroma_appear, beer_style))+
  geom_point()+
  geom_point(aes(x = avg_overall), color = 'red', alpha = .3)+
  labs(x = "Average of Aroma and Appearance Scores",
       y = "")

aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_aroma))%>%
  top_n(25, avg_aroma)%>%
  ggplot(aes(avg_aroma, beer_style))+
  geom_point()+
  labs(x = "Average of Aroma Score",
       y = "")

aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_appearance))%>%
  top_n(25, avg_appearance)%>%
  ggplot(aes(avg_appearance, beer_style))+
  geom_point()+
  labs(x = "Average of Appearance Score",
       y = "")
```

This plot of style is not too surprising if you are a beer snob. American and Russian 
imperial stouts are both highly fragrant, richly colored beers--which happen to be delicious. And a Quad
is similarly known for it's aroma and beautiful color. This seems to be a good representation of
fragrant beers with rich, pleasing appearances in the glass.


#### Code Appendix
This section includes additional code that was used--mainly to explore the data--but that didn't make it past the cutting stage of
report writing. It's included because it's interesting, if not directly relevant to the analysis. 
```{r, code-appendix}
# A look at how often beers from the 'macro' breweries are reviewed
reviews_raw%>%
  filter(brewery_name %in% c("Anheuser-Busch", "Coors Brewing Company", "Miller Brewing Co.", "Heineken Nederland B.V."))%>%
  count(brewery_id, brewery_name, sort = TRUE)

# Plotting distribution of 5 different review categories
reviews_raw %>%
  select(starts_with('review'), -review_profilename)%>%
  gather(review_type, score, -review_time)%>%
  ggplot(aes(score))+
  geom_histogram()+
  facet_wrap(~review_type)+
  scale_y_continuous(labels = number_format(), limits = c(0, 700000), expand = c(0, 0))+
  labs(y = "",
       title = "Distribution of Review Scores",
       subtitle = "Scores all have similar shapes, though Appearance may be a tad narrower")

# A look at percentage of a breweries beers above 15% ABV
beers_above_fifteen_perc <- reviews_raw %>%
  filter(!is.na(beer_abv), !is.na(brewery_name))%>%
  #alter the brewery name again like above to uniquely identify those duplicate brewery names
  mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
                               brewery_name))%>%
  distinct(brewery_name,
           beer_name,
           .keep_all = TRUE)%>%
  group_by(brewery_name)%>%
  summarise(beers = n(),
            beers_above_15 = sum(beer_abv >=15),
            per_abv_15= beers_above_15/beers)

beers_above_fifteen_perc%>%
  filter(beers >= 10)%>%
  mutate(brewery_name = fct_reorder(brewery_name, per_abv_15))%>%
  top_n(25, per_abv_15)%>%
  ggplot(aes(per_abv_15, brewery_name))+
  geom_point()+
  scale_x_continuous(labels = percent_format())+
  expand_limits(x = .02)+
  labs(x = "Percentage of Beers Above 15% ABV",
       y = "",
       title = "Percentage of Breweries Beers which are greater than 15% ABV",
       subtitle = "For breweries producing more than 10 different beers")
```

