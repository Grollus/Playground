---
title: "Analysis of Beer Advocate Reviews Dataset"
subtitle: "Walkthrough of a data science take-home interview test"
output: 
  html_document:
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```

```{r data-load, results = FALSE, message = FALSE}
library(tidyverse)
library(scales)
theme_set(theme_minimal())
# Read in the raw data
reviews_raw <- read_csv("Data/beerreviews/beer_reviews.csv")
```

Interviewing for data science jobs is hard. Since the job definition and responsibilities vary hugely between
companies and roles, you can never quite know what areas of the field you'll be asked about during an interview.
This is stressful for the interviewee and can result in misleading results for the interviewer. 

A while back I stumbled on a great [blog post](https://www.linkedin.com/pulse/how-hire-test-data-skills-one-size-fits-all-interview-tanya-cashorali/) by Tanya
Cashorali where she goes into great detail about why she's taken to using a take-home 'test' to evaluate potential data scientists.
She even lays out a test dataset and problem set for interviews. 

I loved her take on how bad many data science interviews are and I thought it would be great practice(and maybe useful for anyone who stumbles upon this post!) to work through the test she lays out. The test she proposes is simple, and can be answered on many different
levels depending on the candidate. Using a dataset from the popular beer(yay beer!) review website Beer Advocate, you just need to 
answer four analytics questions and present your findings:

1) Which brewery produces the strongest beers by ABV?
2) If you had to pick 3 beers to recommend using only this data, which would you pick?
3) Which of the factors(aroma, taste, appearance, palette) are most important in determining the overall quality of a beer?
4) If I typically enjoy a beer due to its aroma and appearance, which beer style should I try?

This is a really exciting dataset as a beer lover, so without further ado, let's dive in!


### Data Exploration before Analysis

Before answering the questions, I need to get a decent sense of the dataset. `r nrow(reviews_raw%>%distinct(brewery_id))`
```{r}
# 5840 different breweries represented in the dataset: Sam Adams the largest
reviews_raw%>%
  count(brewery_id, brewery_name, sort = TRUE)%>%
  head(8)%>%
  formattable::formattable()

#  66055 different beers reviewed. Lots of Stone, Sierra Nevada representation
reviews_raw%>%
  count(beer_beerid, beer_name, brewery_name, sort = TRUE)

# How many reviews do beers get?
reviews_raw %>%
  count(beer_beerid, beer_name, sort = TRUE)%>%
  ggplot(aes(n))+
  geom_histogram()+
  scale_x_log10()+
  labs(x = "Number of Reviews",
       y = "Number of beers",
       title = "Number of reviews receieved by beers",
       subtitle = "~80% of beers receive 10 or fewer reviews")
  

# removed beers without abv listings and then see how many different beers
# breweries are producing. 
# Also tried to see how that compared to the mean/median # of beers made, but
# that doesn't seem particularly useful
reviews_raw %>%
  filter(!is.na(beer_abv))%>%
  count(brewery_name, beer_name, sort = TRUE)%>%
  count(brewery_name, sort = TRUE)%>%
  mutate(avg_num_beers_made = mean(nn),
         median_num_beers_made = median(nn),
         fraction = nn/avg_num_beers_made)%>%
  arrange(desc(fraction))

# Plotting distribution of 5 different review categories
reviews_raw %>%
  select(starts_with('review'), -review_profilename)%>%
  gather(review_type, score, -review_time)%>%
  ggplot(aes(score))+
  geom_histogram()+
  facet_wrap(~review_type)+
  scale_y_continuous(labels = number_format())

# 104 styles of beer(some seem very close to the same thing)
reviews_raw%>%
  count(beer_style, sort = TRUE)

# Looking at how average score varies by style of beer.
avg_score_by_style <- reviews_raw%>%
  group_by(beer_style)%>%
  summarise(avg_score = mean(review_overall),
            reviews = n())%>%
  ungroup()%>%
  arrange(desc(avg_score))

library(broom)
# This first approach groups by the unique beers and styles--so it averages
# the score of each individual beer and then our final estimates are an average of 
# all the unique beers in that beer style. This givens a lot of weight to beers with
# few reviews

# Have to group differently to show conf intervals for scores of beer styles
by_beerid <- reviews_raw %>%
  group_by(beer_beerid, beer_style)%>%
  summarise(reviews = n(),
            avg_score = mean(review_overall))%>%
  ungroup()%>%
  arrange(desc(reviews))

# generate confidence intervals for styles of beer
beer_style_conf_int <- by_beerid%>%
  add_count(beer_style)%>%
  # filters to beers styles with greater than 250 different beers reviewed
  # may also want to filter to only include specific beers with at least X number of reviews
  filter(n >=250,
         reviews >= 50)%>%
  nest(-beer_style)%>%
  mutate(model = map(data, ~t.test(.$avg_score)))%>%
  unnest(map(model, tidy))

beer_style_conf_int%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(25, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  labs(x = "Average overall review score",
       y = "Beer Style",
       title = "Average review score of different styles of beer",
       subtitle = "Beers grouped by unique beers first, then averaged at the style level")

# This approach creates a unique id for each review(since there isn't one in the dataset)
# and then it groups by beer style. This means I keep all individual reviews and those
# unique beers with very few reviews don't get as much weight. This seems like a 
# really clunky way of doing this, so I am probably missing a much simpler way.

#Assign a unique identifier to each review first
by_reviewid <- reviews_raw %>%
  mutate(unique_id = 1:nrow(reviews_raw))%>%
  mutate(avg_score = review_overall)%>%
  select(unique_id, beer_style, avg_score, beer_beerid)

# generate the conf intervals on the reviewid level data
beer_style_conf_int_by_reviewid <- by_reviewid%>%
  add_count(beer_style)%>%
  # filters to beers styles with greater than 2500 reviews
  filter(n >=2500)%>%
  nest(-beer_style)%>%
  mutate(model = map(data, ~t.test(.$avg_score)))%>%
  unnest(map(model, tidy))

# plot
beer_style_conf_int_by_reviewid%>%
  mutate(beer_style = fct_reorder(beer_style, estimate))%>%
  top_n(25, estimate)%>%
  ggplot(aes(estimate, beer_style))+
  geom_point()+
  geom_errorbarh(aes(xmin = conf.low,
                     xmax = conf.high))+
  labs(x = "Average overall review score",
       y = "Beer Style",
       title = "Average review score of different styles of beer",
       subtitle = "Averaged from individual reviews of beer styles with at least 2500 total reviews")
```


```{r}
# Looking at beer abv; removing reviews with missing abv(about 67k)
reviews_raw %>%
  filter(!is.na(beer_abv))%>%
  count(beer_abv, sort = TRUE)

reviews_raw %>%
  filter(!is.na(beer_abv))%>%
  ggplot(aes(beer_abv))+
  geom_histogram()

# Only 18 beers in the dataset are about 20% abv
reviews_raw%>%
  filter(!is.na(beer_abv), beer_abv > 20)%>%
  count(beer_name, beer_abv, brewery_name, beer_style, sort = TRUE)

reviews_raw%>%
  filter(!is.na(beer_abv), beer_abv > 20)%>%
  ggplot(aes(beer_abv))+
  geom_histogram()

# Most beers hoover around the 5% range which isn't surprising as a beer drinker
reviews_raw %>%
  filter(!is.na(beer_abv), beer_abv < 20)%>%
  ggplot(aes(beer_abv))+
  geom_histogram()
```


### Which brewery produces the strongest beers by ABV%?
This question has more nuance than it appears at first glance. After exploring the 
data, it is clear that some breweries produce some super high abv beers, but very
few people ever try them. Or, a small brewery will produce a very small number of beers
and they will all be very high abv--so perhaps they are specializing in high abv beers.
The question then becomes what are we trying to answer with the question 'which brewery
produces the strongest beers by ABV'?

Ways to approach this:
1. Simple average of abv of all beers produced by a brewery.
2. Threshold breweries to breweries producing > than X number of different
beers. Then average those abvs.
3. Use median to ameliorate the effect of a brewery having just a handful of
really high abv beers skewing their results.
4. Is average(or median) even a good measure of this? It's possible a brewery produces
a lot of high abv beers, but they also produce a ton of more standard fare. Isn't that brewery
a good choice for high abv lovers?
```{r}
average_abv <- reviews_raw %>%
  # remove reviews with missing abv
  filter(!is.na(beer_abv), !is.na(brewery_name))%>%
  # don't want a weighted average, so we grab a df of unique beers for all breweries
  distinct(brewery_id, beer_name, .keep_all = TRUE)%>%
  # group them by the brewery and the beer
  group_by(brewery_id, brewery_name)%>%
  # just doing a simple average here(very debatable) and grabbing the # of unique beers produced by brewery
  summarise(avg_abv = mean(beer_abv),
            med_abv = median(beer_abv),
            num_beers = n()) %>%
  ungroup()%>%
  # optional filtering step here to weed out breweries only producing a small number of beers
  # filter(num_beers >=10)%>%
  mutate(mean_beers_produced_overall = mean(num_beers))
 

# several brewery_names aren't unique; append the unique id to those so identify
duplicate_names <- average_abv %>%
  filter(duplicated(.[["brewery_name"]]))%>%
  select(brewery_name)

average_abv <- average_abv %>%
  mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
                               brewery_name))

average_abv %>%
  mutate(brewery_name = fct_reorder(brewery_name, avg_abv))%>%
  filter(num_beers >= 7)%>%
  top_n(25, avg_abv)%>%
  ggplot(aes(avg_abv, brewery_name))+
  geom_point()+
  labs(x = "%ABV",
       y = "", 
       title = "Top 25 Breweries producing the highest average abv beers",
       subtitle = "Only Breweries producing 7 or more different beers")

average_abv %>%
  distinct(brewery_name, .keep_all = TRUE)%>%
  mutate(brewery_name = fct_reorder(brewery_name, med_abv))%>%
  filter(num_beers >= 7)%>%
  top_n(25, med_abv)%>%
  ggplot(aes(med_abv, brewery_name))+
  geom_point()+
  labs(x = "%ABV",
       y = "", 
       title = "Top 25 Breweries producing the highest median abv beers",
       subtitle = "Only Breweries producing 7 or more different beers")
```

Shorschbrau brewery seems to be head and shoulders above the others in average and median
abv of the beers they produce. With a little googling, it seems they are well known
for pushing the limits of strong beers. They even produce one that is 57% abv!
```{r}
reviews_raw%>%
  filter(!is.na(beer_abv), brewery_name == "Schorschbräu")%>%
  distinct(beer_name, .keep_all = TRUE)%>%
  select(beer_name, beer_abv)
```

Also want to look at those beers above, say 10% abv. Is there some brewery that produces a lot of
those, but gets diluted because they produce so many other 'normal' beers--something like Sam
Adams perhaps.

```{r}
beers_above_ten_perc <- reviews_raw %>%
  filter(!is.na(beer_abv), !is.na(brewery_name))%>%
  #alter the brewery name again like above to uniquely identify those duplicate brewery names
  mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
                               brewery_name))%>%
  distinct(brewery_name,
           beer_name,
           .keep_all = TRUE)%>%
  group_by(brewery_name)%>%
  summarise(beers = n(),
            beers_above_10 = sum(beer_abv >=10),
            per_abv_10 = beers_above_10/beers)

beers_above_ten_perc%>%
  filter(beers >= 20)%>%
  mutate(brewery_name = fct_reorder(brewery_name, per_abv_10))%>%
  top_n(25, per_abv_10)%>%
  ggplot(aes(per_abv_10, brewery_name))+
  geom_point()+
  scale_x_continuous(labels = percent_format())+
  expand_limits(x = .3)+
  labs(x = "Percentage of Beers Above 10% ABV",
       y = "",
       title = "Percentage of Breweries Beers which are greater than 10% ABV",
       subtitle = "For breweries producing more than 20 different beers")



# beers_above_fifteen_perc <- reviews_raw %>%
#   filter(!is.na(beer_abv), !is.na(brewery_name))%>%
#   #alter the brewery name again like above to uniquely identify those duplicate brewery names
#   mutate(brewery_name = ifelse(brewery_name %in% duplicate_names$brewery_name, paste0(brewery_name, "(", brewery_id, ")"),
#                                brewery_name))%>%
#   distinct(brewery_name,
#            beer_name,
#            .keep_all = TRUE)%>%
#   group_by(brewery_name)%>%
#   summarise(beers = n(),
#             beers_above_15 = sum(beer_abv >=15),
#             per_abv_15= beers_above_15/beers)
# 
# beers_above_fifteen_perc%>%
#   filter(beers >= 15)%>%
#   mutate(brewery_name = fct_reorder(brewery_name, per_abv_15))%>%
#   top_n(25, per_abv_15)%>%
#   ggplot(aes(per_abv_15, brewery_name))+
#   geom_point()+
#   scale_x_continuous(labels = percent_format())+
#   expand_limits(x = .02)+
#   labs(x = "Percentage of Beers Above 15% ABV",
#        y = "",
#        title = "Percentage of Breweries Beers which are greater than 15% ABV",
#        subtitle = "For breweries producing more than 15 different beers")
```

If we are looking for breweries that produce a lot of high abv beers in general, 
AleSmith Brewery Company is a good choice with over 70% of the beers they produce
being 10% ABV or higher.


###If you had to pick 3 beers to recommend using only this data, which would you pick?

1. A simple way to do a recommendation is to look at reviewers(and beers) that had a good amount
of reviews. From those you could pick the 3 highest rated beers (by overall rating?)
2. A recommender system based on matrix factorization

```{r}
reviews_raw %>%
  count(review_profilename, sort = TRUE)%>%
  ggplot(aes(n))+
  geom_histogram()+
  scale_x_log10()

# filter to reviewers who have reviewed above 50 beers and beers with greater than 100 reviews
over_50_reviews <- reviews_raw%>%
  add_count(review_profilename)%>%
  filter(n >= 50)%>%
  add_count(beer_beerid)%>%
  rename(num_reviews = nn)%>%
  filter(num_reviews >=100, 
         !is.na(beer_name),
         !is.na(review_profilename),
         !is.na(review_overall))%>%
  # add new user_id(starting at 1) and beer_id(starting at 1)
  mutate(., user_id = group_indices(., review_profilename),
         beer_id = group_indices(., beer_beerid))


user_beer_triplet <- over_50_reviews %>%
  select(user_id, beer_id, review_overall)

library(recosystem)
r <- Reco()
train_set = data_memory(user_index = user_beer_triplet$user_id,
                        item_index = user_beer_triplet$beer_id,
                        rating = user_beer_triplet$review_overall,
                        index1 = TRUE)
# dim parameter controls the number of latent factors in the model.
# Default dim = 10 and that's too few--you end up having pretty much
# the same beers recommended to everyone. There is a balancing to be done here
# of creating too many latent factors and essentially overfitting and too few
# and giving really generic recommendations
r$train(train_set, opts = list(dim = 50,                        
                               costp_l1 = 0, costp_l2 = 0.01,   
                               costq_l1 = 0, costq_l2 = 0.01,   
                               niter = 20,                      
                               nthread = 6))


top_three_beers_for_user <- function(user_beer_sparse = user_beer_triplet, user_id = NULL){
  
  # for this proof of concept implementation, just grabbing a random user_id,
  # but I want to be able to grab a specific user too which is what the if statement
  # is for.
  if (is.null(user_id)){
    user <- sample.int(n_distinct(user_beer_sparse$beer_id), 1)
  } else{
    stopifnot(is.numeric(user_id))
    stopifnot(user_id != 0)
    user <- user_id
  }
  
  # grab the number of unique beers to build grid for prediction
  beer <- 1:n_distinct(user_beer_sparse$beer_id)
  
  #build grid of beers to predict for user
  # this doesn't filter out beers they already have reviewed. it predicts them 
  # anyway.This predict can be used later to gauge the accuracy/usefulness of
  # the recommender.
  
  pred <- expand.grid(user = user, beer = beer)
  # build recosystem object used for prediction
  prediction_set <- data_memory(pred$user, 
                                pred$beer, 
                                index1 = TRUE)
  # do the actual predictions and add to prediction grid
  pred$rating <- r$predict(prediction_set, out_memory())
  
  # df of beers the specific user has reviewed
  users_beers <- user_beer_sparse %>%
    filter(user_id %in% user)
  
  # return top three beers by predicted rating
  # remove predictions for beers user already reviewed
  pred %>%
    filter(!(beer %in% users_beers$beer_id))%>%
    arrange(desc(rating))%>%
    # this join allows me to get the beer name, style and brewery name
    # the prediction df is just id numbers and a rating prediction
    inner_join((over_50_reviews%>%
                 distinct(beer_id, .keep_all = TRUE)%>%
                  select(beer_id, beer_name, beer_style, brewery_name)), by = c("beer" = "beer_id"))%>%
    top_n(3, rating)
}

users_top_rated_beers <- function(df = over_50_reviews, user = NULL){
  # function takes a specific user_id and returns the style, name and rating of their top 10 beers
  df%>%
    filter(user_id == user)%>%
    select(user_id, beer_id, beer_name, beer_style, review_overall)%>%
    top_n(10, review_overall)%>%
    arrange(desc(review_overall))
}

```



### Which of the factors(aroma, taste, appearance, palette) are most important 
in determining the overall quality of the beer?

Off the top of my head I can think of two ways to approach this:
1. I can look at the correlation of the 4 sub factors with the overall review
score of a beer.
2. I can build a linear model to see which factors are significant when predicting 
the overall quality of a beer.
  2a. This could be problematic: Think Bayesian Richard McElrith
  


```{r}
library(corrr)
reviews_raw %>%
  select(review_appearance, review_aroma, review_palate, review_taste, review_overall)%>%
  correlate()
```


```{r}
m1 <- lm(review_overall~ review_appearance + review_aroma + review_taste + review_palate, data = reviews_raw)
summary(m1)

m2 <- lm(review_overall ~ review_appearance, data = reviews_raw)
summary(m2)
m3 <- lm(review_overall ~ review_aroma, data = reviews_raw)
summary(m3)
m4 <- lm(review_overall ~ review_taste, data = reviews_raw)
summary(m4)
m5 <- lm(review_overall ~ review_palate, data = reviews_raw)
summary(m5)


```






### If I enjoy a beer due to its aroma and appearance, which beer style should I try?
Again, there are several ways to approach this question.
1. Look at which beers styles have high aroma and appearance reviews and recommend those.
2. Look at aroma and appearance above some threshold value and then recommend the most
highly rated beer style overall amongst those high aroma and appearance scores.



```{r}
aroma_and_appearance <- reviews_raw %>%
  filter(!is.na(brewery_name))%>%
  group_by(beer_style)%>%
  summarise(reviews = n(),
            avg_aroma = mean(review_aroma),
            avg_appearance = mean(review_appearance),
            avg_overall = mean(review_overall))%>%
  ungroup()%>%
  mutate(total_aroma_appear = avg_aroma + avg_appearance,
         avg_aroma_appear = (avg_aroma + avg_appearance)/2)%>%
  arrange(desc(avg_aroma_appear), desc(avg_overall))


aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_aroma_appear))%>%
  top_n(25, avg_aroma_appear)%>%
  ggplot(aes(avg_aroma_appear, beer_style))+
  geom_point()+
  geom_point(aes(x = avg_overall), color = 'red', alpha = .3)+
  labs(x = "Average of Aroma and Appearance Scores",
       y = "")

aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_aroma))%>%
  top_n(25, avg_aroma)%>%
  ggplot(aes(avg_aroma, beer_style))+
  geom_point()+
  labs(x = "Average of Aroma Score",
       y = "")

aroma_and_appearance %>%
  mutate(beer_style = fct_reorder(beer_style, avg_appearance))%>%
  top_n(25, avg_appearance)%>%
  ggplot(aes(avg_appearance, beer_style))+
  geom_point()+
  labs(x = "Average of Appearance Score",
       y = "")
```

This plot of style is not too surprising if you are a beer snob. American and Russian 
imperial stouts are both highly fragrant, richly colored beers--which happen to be delicious. And a Quad
is similarly known for it's aroma and beautiful color. This seems to be a good representation of
fragrant beers with rich, pleasing appearances in the glass.


