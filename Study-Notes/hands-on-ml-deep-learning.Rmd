---
title: "Hands-on ML Deep Learning"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(dplyr)
library(keras) # for fitting DNN

library(tfruns) # for additional grid search and model training functions

library(tfestimators) # provides grid search and model training interface
```

Have to one-hot encode response since it is multinomial. Also have to one-hot encode categorical
features of the feature set. Neural networks also sensitive to scale of data, so needs to be standardized.
```{r}
# Import MNIST training data
mnist <- dslabs::read_mnist()
mnist_x <- mnist$train$images
mnist_y <- mnist$train$labels

# Rename columns and standardize feature values
colnames(mnist_x) <- paste0("V", 1:ncol(mnist_x))
mnist_x <- mnist_x / 255

# One-hot encode response
mnist_y <- to_categorical(mnist_y, 10)
```

In Keras, you build layers
```{r}
model <- keras_model_sequential()%>%
  layer_dense(units = 128, input_shape = ncol(mnist_x))%>%
  layer_dense(units = 64)%>%
  layer_dense(units = 10)

# Adding activation functions to the model
model <- keras_model_sequential()%>%
  layer_dense(units = 128, activation = 'relu', input_shape = ncol(mnist_x))%>%
  layer_dense(units = 64, activation = 'relu')%>%
  layer_dense(units = 10, activation = 'softmax')
```

## Backpropagation
To include backpropagation, you compile in the code sequence.
```{r}
model <- keras_model_sequential()%>%
  layer_dense(units = 128, activation = 'relu', input_shape = ncol(mnist_x))%>%
  layer_dense(units = 64, activation = 'relu')%>%
  layer_dense(units = 10, activation = 'softmax')%>%
  #backpropogation
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_rmsprop(),
    metrics = c("accuracy")
  )
```

To train this model, we use fit function.
```{r}
# train the model
fit1 <- model %>%
  fit(
  x = mnist_x,
  y = mnist_y,
  epochs = 25,
  batch_size = 128,
  validation_split = .2,
  verbose = TRUE)

fit1
plot(fit1)
```

## Batch Normalization
```{r}
model_w_norm <- keras_model_sequential() %>%
  
  # Network architecture with batch normalization
  layer_dense(units = 256, activation = "relu", input_shape = ncol(mnist_x)) %>%
  layer_batch_normalization() %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dense(units = 10, activation = "softmax") %>%

  # Backpropagation
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(),
    metrics = c("accuracy")
  )

fit2 <- model_w_norm %>%
  fit(
  x = mnist_x,
  y = mnist_y,
  epochs = 25,
  batch_size = 128,
  validation_split = .2,
  verbose = TRUE)
```

## Regularization
```{r}
model_w_reg <- keras_model_sequential()%>%
  
  # Network architexture with L1 regularization and batch normalization
  layer_dense(units = 256, activation = 'relu', input_shape = ncol(mnist_x),
              kernel_regularizer = regularizer_l2(0.001))%>%
  layer_batch_normalization()%>%
  layer_dense(units = 128, activation = 'relu',
              kernel_regularizer = regularizer_l2(0.001))%>%
  layer_batch_normalization()%>%
  layer_dense(units = 64, activation = 'relu',
              kernel_regularizer = regularizer_l2(0.001))%>%
  layer_batch_normalization()%>%
  layer_dense(units = 10, activation = 'softmax')%>%
  
  #backprop
  compile(
    loss = 'categorical_crossentropy',
    optimizer =  optimizer_rmsprop(),
    metrics = c("accuracy")
  )

```

## Dropout
Dropout is added with a *layer_dropout* call between layers

```{r}
model_w_drop <- keras_model_sequential() %>%
  
  # Network architecture with 20% dropout
  layer_dense(units = 256, activation = "relu", input_shape = ncol(mnist_x)) %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = "softmax") %>%

  # Backpropagation
  compile(
    loss = "categorical_crossentropy",
    optimizer = optimizer_rmsprop(),
    metrics = c("accuracy")
  )

```

## Optimal model using Adam optimizer and reduction of learning rate

```{r}
model_w_adj_lrn <- keras_model_sequential() %>%
  layer_dense(units = 256, activation = "relu", input_shape = ncol(mnist_x)) %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.4) %>%
  layer_dense(units = 128, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 64, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = 0.2) %>%
  layer_dense(units = 10, activation = "softmax") %>%
  compile(
    loss = 'categorical_crossentropy',
    optimizer = optimizer_adam(),
    metrics = c('accuracy')
  ) %>%
  fit(
    x = mnist_x,
    y = mnist_y,
    epochs = 35,
    batch_size = 128,
    validation_split = 0.2,
    callbacks = list(
      callback_early_stopping(patience = 5),
      callback_reduce_lr_on_plateau(factor = 0.05)
      ),
    verbose = FALSE
  )

model_w_adj_lrn

# Optimal
min(model_w_adj_lrn$metrics$val_loss)

max(model_w_adj_lrn$metrics$val_acc)

# Learning rate
plot(model_w_adj_lrn)
```

## Grid Search 

```{r}
# Keras based grid search
FLAGS <- flags(
  # Nodes
  flag_numeric("nodes1", 256),
  flag_numeric("nodes2", 128),
  flag_numeric("nodes3", 64),
  # Dropout
  flag_numeric("dropout1", 0.4),
  flag_numeric("dropout2", 0.3),
  flag_numeric("dropout3", 0.2),
  # Learning paramaters
  flag_string("optimizer", "rmsprop"),
  flag_numeric("lr_annealing", 0.1)
)

model <- keras_model_sequential() %>%
  layer_dense(units = FLAGS$nodes1, activation = "relu", input_shape = ncol(mnist_x)) %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = FLAGS$dropout1) %>%
  layer_dense(units = FLAGS$nodes2, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = FLAGS$dropout2) %>%
  layer_dense(units = FLAGS$nodes3, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = FLAGS$dropout3) %>%
  layer_dense(units = 10, activation = "softmax") %>%
  compile(
    loss = 'categorical_crossentropy',
    metrics = c('accuracy'),
    optimizer = FLAGS$optimizer
  ) %>%
  fit(
    x = mnist_x,
    y = mnist_y,
    epochs = 35,
    batch_size = 128,
    validation_split = 0.2,
    callbacks = list(
      callback_early_stopping(patience = 5),
      callback_reduce_lr_on_plateau(factor = FLAGS$lr_annealing)
    ),
    verbose = FALSE
  )


# Run various combinations of dropout1 and dropout2
runs <- tuning_run("scripts/mnist-grid-search.R", 
  flags = list(
    nodes1 = c(64, 128, 256),
    nodes2 = c(64, 128, 256),
    nodes3 = c(64, 128, 256),
    dropout1 = c(0.2, 0.3, 0.4),
    dropout2 = c(0.2, 0.3, 0.4),
    dropout3 = c(0.2, 0.3, 0.4),
    optimizer = c("rmsprop", "adam"),
    lr_annealing = c(0.1, 0.05)
  ),
  sample = 0.05
)

runs %>% 
  filter(metric_val_loss == min(metric_val_loss)) %>% 
  glimpse()
```

